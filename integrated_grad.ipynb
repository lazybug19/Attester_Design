import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from tensorflow.keras.models import load_model
print(len(feature_names))
model = load_model("IOT_CNN.h5")
def integrated_gradients(inputs, model, baseline=None, steps=50):
    """
    Compute Integrated Gradients for a given input instance and model.
    """
    if baseline is None:
        baseline = tf.zeros(shape=inputs.shape)#change as needed, zero will give you true neutral, but you can implement average values too if you want
    inputs = tf.cast(inputs, tf.float32)
    baseline = tf.cast(baseline, tf.float32)

    # Generate scaled inputs between baseline and the input instance
    interpolated_inputs = [baseline + (float(i) / steps) * (inputs - baseline) for i in range(steps + 1)]
    #generic SO formula, tobe changed if better
    accumulated_grads = tf.zeros_like(inputs[0])

    for i, interpolated_input in enumerate(interpolated_inputs):
        with tf.GradientTape() as tape:
            # Watch the interpolated input for gradient computation
            tape.watch(interpolated_input)
            # Get model prediction for the interpolated input
            prediction = model(interpolated_input)

        # Compute the gradient of the prediction with respect to the interpolated input
        grads = tape.gradient(prediction, interpolated_input)
        accumulated_grads += grads / steps  # Accumulate and average gradients

    # Compute Integrated Gradients by multiplying with input minus baseline
    integrated_gradients = (inputs - baseline) * accumulated_grads
    return integrated_gradients

# Select a sample input to explain
sample_input = X_test[8:9]
attributions = integrated_gradients(sample_input, model)

# Extract attribution scores and store them in a dictionary with actual feature names
attribution_scores = attributions[0].numpy().flatten()#need this to be a 1D array
feature_attributions = {feature: score for feature, score in zip(feature_names, attribution_scores)}
feature_count = len(feature_names)  # 61 in this case
next_square_dim = int(np.ceil(np.sqrt(feature_count)))  # make it a perfect square
perfect_square_size = next_square_dim * next_square_dim  
padded_attributions = np.pad(attribution_scores, (0, perfect_square_size - len(attribution_scores)), 'constant')

# Reshape to next_square_dim x next_square_dim for heatmap
attr_matrix = padded_attributions.reshape(next_square_dim, next_square_dim)
plt.imshow(attr_matrix, cmap='viridis', interpolation='nearest')
plt.colorbar(label='Attribution Score')
plt.title("Feature Importance using Integrated Gradients")
plt.xlabel("Feature Dimension 1")
plt.ylabel("Feature Dimension 2")
plt.xticks(range(next_square_dim), [f"F{i+1}" for i in range(next_square_dim)], rotation=90)
plt.yticks(range(next_square_dim), [f"F{i+1}" for i in range(next_square_dim)])
plt.show()
